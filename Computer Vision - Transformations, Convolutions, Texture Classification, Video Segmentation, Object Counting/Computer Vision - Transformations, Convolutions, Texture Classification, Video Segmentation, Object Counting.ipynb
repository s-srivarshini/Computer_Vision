{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbdc9376-ab89-4df5-a132-413eb4719170",
   "metadata": {},
   "source": [
    "# ECS709 - Introduction to Computer Vision\n",
    "## Coursework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff890d51-f233-4e72-8fdb-27b315d46a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663bec2-3876-4b78-ab29-7a2fb813f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738c8c1-46cf-46a2-83e8-a752f7e63f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bcff9a-490f-435c-a988-ed2ebe1b43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cfb213-4a30-49c8-8411-531cdc2e43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python --no-binary opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb302fe-06db-45df-9868-6bcab21e907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e7ec1-2fe9-4457-8f6e-7bee4be041fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c0c9b-48c2-4749-b063-06c3831aba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28bfc11-47ad-498e-b5cc-29ab17e5e661",
   "metadata": {},
   "source": [
    "## 1) Transformations.\n",
    "<br>Rotation, translation and skew are useful operations for matching, tracking, and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a9e84-35a5-40ff-845c-495900df118b",
   "metadata": {},
   "source": [
    "#### a) Write a function that takes as input an image I, rotates it by an angle θ1 and horizontally skews it by an angle, θ2. Write the matrix formulation for image rotation R(.) and skewing S(.). Define all the variables. Note that the origin of the coordinate system of the programming environment you use might be different from the one shown in the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd8d43-09e6-402c-91f1-155fec8d2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e781f-0669-4e09-970b-e6321e34ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_with_interpolation(main_img, angle_of_rotation, rotation_point):\n",
    "    angle_of_rotation = np.radians(angle_of_rotation)\n",
    "    h, w = main_img.shape[:2]\n",
    "    rotation_point_x, rotation_point_y = rotation_point\n",
    "\n",
    "    # Calculate the new dimensions for the rotated image\n",
    "    cos_theta = np.abs(np.cos(angle_of_rotation))\n",
    "    sin_theta = np.abs(np.sin(angle_of_rotation))\n",
    "    new_w = int(h * sin_theta + w * cos_theta)\n",
    "    new_h = int(h * cos_theta + w * sin_theta)\n",
    "\n",
    "    new_img = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n",
    "\n",
    "    for new_y in range(new_h):\n",
    "        for new_x in range(new_w):\n",
    "            x = (new_x - new_w // 2) * np.cos(angle_of_rotation) + (new_y - new_h // 2) * np.sin(angle_of_rotation) + rotation_point_x\n",
    "            y = -(new_x - new_w // 2) * np.sin(angle_of_rotation) + (new_y - new_h // 2) * np.cos(angle_of_rotation) + rotation_point_y\n",
    "\n",
    "            if 0 <= x < w - 1 and 0 <= y < h - 1:\n",
    "                x0 = int(x)\n",
    "                y0 = int(y)\n",
    "                x1 = x0 + 1\n",
    "                y1 = y0 + 1\n",
    "\n",
    "                # Bilinear interpolation\n",
    "                dx = x - x0\n",
    "                dy = y - y0\n",
    "\n",
    "                for channel in range(3):  # For RGB channels\n",
    "                    new_img[new_y, new_x, channel] = (\n",
    "                        (1 - dx) * (1 - dy) * main_img[y0, x0, channel] +\n",
    "                        dx * (1 - dy) * main_img[y0, x1, channel] +\n",
    "                        (1 - dx) * dy * main_img[y1, x0, channel] +\n",
    "                        dx * dy * main_img[y1, x1, channel]\n",
    "                    )\n",
    "\n",
    "    # Find the bounding box of the rotated image\n",
    "    non_zero_indices = np.nonzero(new_img)\n",
    "    min_y, max_y = np.min(non_zero_indices[0]), np.max(non_zero_indices[0])\n",
    "    min_x, max_x = np.min(non_zero_indices[1]), np.max(non_zero_indices[1])\n",
    "\n",
    "    # Calculate the dimensions of the rotated image without the background frame\n",
    "    rotated_h = max_y - min_y + 1\n",
    "    rotated_w = max_x - min_x + 1\n",
    "\n",
    "    return new_img[min_y:max_y+1, min_x:max_x+1, :]\n",
    "\n",
    "img = cv2.imread('cat.jpg')\n",
    "height, width = img.shape[:2]\n",
    "rotation_point = (width // 2, height // 2)\n",
    "angle = int(input(\"Enter angle in degrees:\"))  # Convert angle to radians\n",
    "\n",
    "rotated_img = rotate_with_interpolation(img, angle, rotation_point)\n",
    "\n",
    "plt.imshow(rotated_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc06f1-7805-41db-a38f-deddb7072974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shear_with_interpolation(main_img, shear_angle):\n",
    "    h, w = main_img.shape[:2]\n",
    "\n",
    "    shear_matrix = np.array([\n",
    "        [1, np.tan(np.radians(shear_angle))],\n",
    "        [0, 1]\n",
    "    ])\n",
    "\n",
    "    new_w = int(w + abs(np.tan(np.radians(shear_angle))) * h)\n",
    "    new_h = h\n",
    "\n",
    "    new_img = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n",
    "\n",
    "    for new_y in range(new_h):\n",
    "        for new_x in range(new_w):\n",
    "            original_coords = np.dot(np.linalg.inv(shear_matrix), np.array([new_x, new_y]))\n",
    "            x = original_coords[0]\n",
    "            y = original_coords[1]\n",
    "\n",
    "            if 0 <= x < w - 1 and 0 <= y < h - 1:\n",
    "                x0 = int(x)\n",
    "                y0 = int(y)\n",
    "                x1 = x0 + 1\n",
    "                y1 = y0 + 1\n",
    "\n",
    "                # Bilinear interpolation\n",
    "                dx = x - x0\n",
    "                dy = y - y0\n",
    "\n",
    "                for channel in range(3):  # For RGB channels\n",
    "                    new_img[new_y, new_x, channel] = (\n",
    "                        (1 - dx) * (1 - dy) * main_img[y0, x0, channel] +\n",
    "                        dx * (1 - dy) * main_img[y0, x1, channel] +\n",
    "                        (1 - dx) * dy * main_img[y1, x0, channel] +\n",
    "                        dx * dy * main_img[y1, x1, channel]\n",
    "                    )\n",
    "\n",
    "    return new_img\n",
    "\n",
    "img = cv2.imread('cat.jpg')\n",
    "height, width = img.shape[:2]\n",
    "\n",
    "shear_angle = float(input(\"Enter shear angle in degrees: \"))\n",
    "\n",
    "sheared_img = shear_with_interpolation(img, shear_angle)\n",
    "\n",
    "plt.imshow(sheared_img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45a3a25-2e63-44d3-a11f-03d7c20a2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('cat.jpg')\n",
    "height, width = img.shape[:2]\n",
    "rotation_point = (width // 2, height // 2)\n",
    "angle = np.radians(int(input(\"Enter rotation angle in degrees: \")))  # Convert angle to radians\n",
    "shear_angle = float(input(\"Enter shear angle in degrees: \"))\n",
    "rotated_img = rotate_with_interpolation(img, angle, rotation_point)\n",
    "sheared_img = shear_with_interpolation(rotated_img, shear_angle)\n",
    "\n",
    "plt.imshow(sheared_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f66355-e5cc-4f2d-bdae-d26ef1e1791c",
   "metadata": {},
   "source": [
    "#### b) Create an image that contains your name written in Arial, point 72, capital letters. Rotate clockwise the image you created by 30, 60, 120 and -50 degrees. Skew the same image by 10, 40 and 60 degrees. Complete the process so that all the pixels have a value. Discuss in the report the advantages and disadvantages of different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57e62c-3e92-4dd6-808e-58c3a5337a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "text = \"Nikita\"\n",
    "font = {'family': 'Arial', 'weight': 'normal', 'size': 72}\n",
    "\n",
    "plt.text(0.5, 0.5, text, ha='center', va='center', **font)\n",
    "# Remove the x-axis\n",
    "plt.gca().xaxis.set_visible(False)\n",
    "\n",
    "# Remove the y-axis\n",
    "plt.gca().yaxis.set_visible(False)\n",
    "plt.savefig('my_name.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7d988-05da-4e08-bd89-06466eb3bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('my_name.jpg')\n",
    "rotation_angle = [30, 60, 120, -50]\n",
    "shear_angle = [10, 40, 60]\n",
    "for i in rotation_angle:\n",
    "    height, width = img.shape[:2]\n",
    "    rotation_point = (width // 2, height // 2)\n",
    "    rotated_img = rotate_with_interpolation(img, i, rotation_point)\n",
    "    plt.imshow(rotated_img)\n",
    "    plt.show()\n",
    "\n",
    "for j in shear_angle:\n",
    "    sheared_img = shear_with_interpolation(img,j,)\n",
    "    plt.imshow(sheared_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c1760-fdd9-4b80-aaf7-a16b9d066151",
   "metadata": {},
   "source": [
    "#### c) Analyse the results when you change the order of the two operators: R(S(I)) and S(R(I)).\n",
    "#### i) Rotate the image by θ1 = 20 clockwise and then skew the result by θ2 = 50. \n",
    "#### ii) Skew the image by θ2 = 50 and then rotate the result by θ1 = 20 clockwise.\n",
    "#### Are the results of (i) and (ii) the same? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aea772-79ec-4b19-8383-35015304e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('cat.jpg')\n",
    "height, width = img.shape[:2]\n",
    "rotation_point = (width // 2, height // 2)\n",
    "angle = 20  # Convert angle to radians\n",
    "shear_angle = 50\n",
    "rotated_img = rotate_with_interpolation(img, angle, rotation_point)\n",
    "sheared_img = shear_with_interpolation(rotated_img, shear_angle)\n",
    "\n",
    "plt.imshow(sheared_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c51718-d37b-4dad-a2e5-674448b52014",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('cat.jpg')\n",
    "height, width = img.shape[:2]\n",
    "rotation_point = (width // 2, height // 2)\n",
    "shear_angle = 50\n",
    "angle = 20  # Convert angle to radians\n",
    "sheared_img = shear_with_interpolation(rotated_img, shear_angle)\n",
    "rotated_img = rotate_with_interpolation(img, angle, rotation_point)\n",
    "\n",
    "\n",
    "plt.imshow(sheared_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5cb3dd-b6c1-47ce-8fc1-6fef5a0c9024",
   "metadata": {},
   "source": [
    "## 2) Convolution. (Use Dataset A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f09044-5e31-4927-b513-8e270018928c",
   "metadata": {},
   "source": [
    "#### Convolution provides a way of multiplying two arrays to produce a third array. Depending on the designed filter and the intended effect, the kernel can be a matrix of dimensions, for example, 3x3, 5x5 or 7x7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517d88c-f513-43ba-9a63-a92b64c011e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0390f0eb-4823-462d-969a-1512d5bae0dd",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9b7ea-e40e-415f-a775-dad94733e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_grayscale(input_image, kernel):\n",
    "    # height and width of input image\n",
    "    input_image_height, input_image_width = input_image.shape\n",
    "    # height and width of the kernel\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "    # Calculating the padding required around the image\n",
    "    pad_height = kernel_height // 2\n",
    "    pad_width = kernel_width // 2\n",
    "\n",
    "    # Padding the image\n",
    "    padded_input_image = np.pad(input_image, ((pad_height, pad_height), (pad_width, pad_width)), mode='constant')\n",
    "\n",
    "    # Create an empty output_image\n",
    "    output_image = np.zeros_like(input_image)\n",
    "\n",
    "    # Convolution operation formula -> h(n) = f(n) * g(n)\n",
    "    for i in range(input_image_height):\n",
    "        for j in range(input_image_width):\n",
    "            output_image[i, j] = np.sum(padded_input_image[i:i + kernel_height, j:j + kernel_width] * kernel)\n",
    "\n",
    "    # Normalize the output_image to achieve smoother results\n",
    "    output_image = (output_image - np.min(output_image)) / (np.max(output_image) - np.min(output_image)) * 255\n",
    "\n",
    "    return output_image.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec65984-ef13-493c-8134-6e40eaa6294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Function for image convolution\n",
    "def apply_convolution(image, kernel):\n",
    "    return convolve2d(image, kernel, mode='same', boundary='symm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686e166-d65d-4467-ba2c-8a402c844ce4",
   "metadata": {},
   "source": [
    "## Display and save function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf2bfa-326c-4b04-a55f-3ddde0e08660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_and_save_image(kernel,path = None,name_of_folder = \"images\"):\n",
    "    if path: \n",
    "        folder_path = path\n",
    "    else:\n",
    "        # Specify the folder containing the images\n",
    "        folder_path = 'Dataset/DatasetA/'\n",
    "\n",
    "    # Get the absolute path for the folder\n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "\n",
    "    # List all image files in the folder with absolute paths\n",
    "    image_files = [os.path.abspath(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "\n",
    "    # Create a directory to save the resulting images\n",
    "    name_of_folder = name_of_folder\n",
    "    output_directory = os.path.join('Dataset/DatasetA/', name_of_folder)\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Iterate through the images, apply convolution, and save the resulting images\n",
    "    for image_file in image_files:\n",
    "        # Load the image\n",
    "        input_image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Perform convolution using the averaging kernel\n",
    "        result_image = apply_convolution(input_image, kernel)\n",
    "\n",
    "        # Display the original and convolved images\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(result_image, cmap='gray')\n",
    "        plt.title('Convolved Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Define the output path for the convoluted image\n",
    "        output_filename = os.path.splitext(os.path.basename(image_file))[0] + '_convoluted.jpg'\n",
    "        output_image_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "        # Save the resulting image with the full output path\n",
    "        cv2.imwrite(output_image_path, result_image)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da533ff6-ba63-4034-8062-69709d3c942c",
   "metadata": {},
   "source": [
    "#### a) Code a function that takes an input image, performs convolution with a given kernel, and returns the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e043f6-6454-493a-9336-13d1a455b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.array([[0, 0, 0],\n",
    "                   [0, 1, 0],\n",
    "                   [0, 0, 0]]) \n",
    "\n",
    "display_and_save_image(kernel,'Dataset/DatasetA/','normal_kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19440c08-1813-4112-b99b-9210a9d069f5",
   "metadata": {},
   "source": [
    "#### b) Design a convolution kernel that computes, for each pixel, the average intensity value in a 3x3 region. Use this kernel and the filtering function above, and save the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f6f4e-81a3-4f31-a52f-5f20d28534f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 3x3 averaging kernel\n",
    "kernel = np.array([[1/9, 1/9, 1/9],\n",
    "                   [1/9, 1/9, 1/9],\n",
    "                   [1/9, 1/9, 1/9]])\n",
    "\n",
    "display_and_save_image(kernel,'Dataset/DatasetA/','average_kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216410a-a584-4fed-b3f6-45553a41cd05",
   "metadata": {},
   "source": [
    "#### c) Use the kernels provided below, apply the filtering function and save the resulting images. Comment on the effect of each kernel.\n",
    "kernel A\n",
    "1 2 1\n",
    "2 4 2\n",
    "1 2 1\n",
    "kernel B\n",
    "0 1 0\n",
    "1 -4 1\n",
    "0 1 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bed680-4a1e-4464-9a75-d0312890db1a",
   "metadata": {},
   "source": [
    "### Applying filtering function(Kernel A) on RGB Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688fdd1-1c10-4437-8f84-ea85bc69f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_A = np.array([[1, 2, 1],\n",
    "                   [2, 4, 2],\n",
    "                   [1, 2, 1]])\n",
    "\n",
    "kernel_A = kernel_A / np.sum(np.abs(kernel_A))\n",
    "\n",
    "display_and_save_image(kernel_A,'Dataset/DatasetA/average_kernel','kernel_A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ae29c-b2b7-4956-8ea7-91c3e10abb4f",
   "metadata": {},
   "source": [
    "### Applying filtering function(Kernel B) on Grayscale Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189d73f-440d-4c86-a0ee-3ccfe17243bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_B = np.array([[0, 1, 0],\n",
    "                   [1, -4, 1],\n",
    "                   [0, 1, 0]])\n",
    "\n",
    "kernel_B = kernel_B / np.sum(np.abs(kernel_B))\n",
    "display_and_save_image(kernel_B,'Dataset/DatasetA/average_kernel','kernel_B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b289da8-4d0e-4bc0-814f-278156d61502",
   "metadata": {},
   "source": [
    "#### d)Use the filtering function for the following filtering operations: (i) A followed by A; (ii) A followed by B; (iii) B followed by A. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb4646-61ac-4d20-84e7-34951a9e0e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_and_save_for_two_kernel_image(kernel_1,kernel_2):\n",
    "    \n",
    "      # Specify the folder containing the images\n",
    "    folder_path = 'Dataset/DatasetA/'\n",
    "\n",
    "    # Get the absolute path for the folder\n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "\n",
    "    # List all image files in the folder with absolute paths\n",
    "    image_files = [os.path.abspath(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "\n",
    "    # Create a directory to save the resulting images\n",
    "    output_directory = os.path.join(folder_path, 'averaged_images')\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Iterate through the images, apply convolution, and save the resulting images\n",
    "    for image_file in image_files:\n",
    "        # Load the image\n",
    "        input_image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "        kernel = np.array([[1/9, 1/9, 1/9],\n",
    "                   [1/9, 1/9, 1/9],\n",
    "                   [1/9, 1/9, 1/9]])\n",
    "\n",
    "        input_input = apply_convolution(input_image,kernel)\n",
    "        # Perform convolution using the first kernel\n",
    "        kernel_1 = kernel_1 / np.sum(np.abs(kernel_1))\n",
    "        result_image_kernel_1 = apply_convolution(input_image, kernel_1)\n",
    "        kernel_2 = kernel_2 / np.sum(np.abs(kernel_2))\n",
    "        # Perform convolution using the second kernel\n",
    "        result_of_two_kernel_image = apply_convolution(result_image_kernel_1, kernel_2)\n",
    "\n",
    "        # Display the original and convolved images\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(result_of_two_kernel_image, cmap='gray')\n",
    "        plt.title('Convolved Image (Two Kernels)')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Define the output path for the convoluted image\n",
    "        output_filename = os.path.splitext(os.path.basename(image_file))[0] + '_convoluted.jpg'\n",
    "        output_image_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "        # Save the resulting image with the full output path\n",
    "        cv2.imwrite(output_image_path, result_of_two_kernel_image)\n",
    "        plt.show()\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b9890d-1458-46cb-bb1a-09dac94705d5",
   "metadata": {},
   "source": [
    "**(i) A followed by A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caced28-bae8-4198-9e4b-e6a6ae2b2a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_save_for_two_kernel_image(kernel_A,kernel_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e62c1d7-bedf-4d64-afd4-c64042d31ffc",
   "metadata": {},
   "source": [
    "**(ii) A followed by B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8795982-d6c0-4057-b89f-892e5b0dcbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_save_for_two_kernel_image(kernel_A,kernel_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f94a3b-3dc3-4875-b1e0-3654b6d4b2ef",
   "metadata": {},
   "source": [
    "**(iii) B followed by A.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02c42b-9a6a-485c-9704-c64348713c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_save_for_two_kernel_image(kernel_B,kernel_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8385d-6540-4636-8c90-32daa71a26b4",
   "metadata": {},
   "source": [
    "### 3) Video Segmentation. (Use Dataset B)\n",
    "A colour histogram h(.) can be generated by counting how many times each colour occurs in an image.\n",
    "Histogram intersection can be used to match a pair of histograms. Given a pair of histograms, e.g., of an \n",
    "input image I and a model M, each containing n bins, the intersection of the histograms is defined as \n",
    "∑ min[h(Ij), h(Mj)]\n",
    "n\n",
    "j=1\n",
    ". \n",
    "\n",
    "#### a)Write a histogram function that returns the colour histogram of an input image. Visualize the histogram  and save the corresponding figure. For a given video sequence, use the above function to construct the histogram of each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f87e01-a05e-43a4-a2b1-620bfd2e4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67c752-f89c-4543-b77f-fdc7ffbb7233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def color_histogram(image):\n",
    "    # Convert the image from BGR to RGB\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Calculate the histogram for each color channel\n",
    "    hist_r = cv2.calcHist([img_rgb], [0], None, [256], [0, 256])  # Red\n",
    "    hist_g = cv2.calcHist([img_rgb], [1], None, [256], [0, 256])  # Green\n",
    "    hist_b = cv2.calcHist([img_rgb], [2], None, [256], [0, 256])  # Blue\n",
    "\n",
    "    # Normalize the histograms\n",
    "    hist_r = hist_r / np.sum(hist_r)\n",
    "    hist_g = hist_g / np.sum(hist_g)\n",
    "    hist_b = hist_b / np.sum(hist_b)\n",
    "\n",
    "    return hist_r, hist_g, hist_b\n",
    "\n",
    "def plot_histogram(frame_lst):\n",
    "    # Plot the histograms\n",
    "    if len(frame_lst) == 1:\n",
    "        hist_r, hist_g, hist_b = color_histogram(frame_lst[0])\n",
    "        plt.plot(hist_r, color='red', label='Red', alpha=0.7)\n",
    "        plt.plot(hist_g, color='green', label='Green', alpha=0.7)\n",
    "        plt.plot(hist_b, color='blue', label='Blue', alpha=0.7)\n",
    "        plt.title('Frame')\n",
    "        plt.savefig(f'Dataset/histogram/color_histogram.png')\n",
    "        plt.xlabel('Pixel Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        if len(frame_lst) != 0:\n",
    "            for i in range(0, len(frame_lst), 5):  # Increment by 6 frames\n",
    "                fig, axs = plt.subplots(1, 5, figsize=(18, 5))\n",
    "                for j in range(5):\n",
    "                    if i + j < len(frame_lst):\n",
    "                        hist_r, hist_g, hist_b = color_histogram(frame_lst[i + j])\n",
    "                        axs[j].plot(hist_r, color='red', label='Red', alpha=0.7)\n",
    "                        axs[j].plot(hist_g, color='green', label='Green', alpha=0.7)\n",
    "                        axs[j].plot(hist_b, color='blue', label='Blue', alpha=0.7)\n",
    "                        axs[j].set_title(f'Frame {i + j}')\n",
    "\n",
    "                # Customize the plot for each row\n",
    "                for ax in axs:\n",
    "                    ax.set_xlabel('Pixel Value')\n",
    "                    ax.set_ylabel('Frequency')\n",
    "                    ax.legend()\n",
    "\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Save the figure for each row\n",
    "                plt.savefig(f'Dataset/histogram/color_histogram_row_{i // 5}.png')\n",
    "\n",
    "                # Show the plot for each row\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857835ab-ffe3-492e-85a4-dcc8d300f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram([cv2.imread(\"test.jpg\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f182d68-56b9-4e75-92ca-c137a921c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "\n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not capture.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return \n",
    "    frame_lst = []\n",
    "    while True:\n",
    "        ret,frame = capture.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        \n",
    "        hist_r,hist_g,hist_b = color_histogram(frame)\n",
    "        frame_lst.append(frame)\n",
    "    plot_histogram(frame_lst)\n",
    "\n",
    "    # Release the video capture object\n",
    "    capture.release()\n",
    "    return frame_lst\n",
    "\n",
    "\n",
    "# Example usage for a video\n",
    "video_path = 'Dataset/DatasetB.avi'\n",
    "frame_lst = process_video(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c5784-d889-4d17-80ae-7122b3fb7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame_lst[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797be03d-6a67-41fa-b181-1dd7f73ffe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame_lst[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29991ba-06de-496f-a38e-14d035e67ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame_lst[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ea68d-d106-4c07-a3ca-a4337c7a7d68",
   "metadata": {},
   "source": [
    "### b) Write a function that returns the value of the intersection of a pair of histograms. For a given video sequence, use the histogram intersection function to calculate the intersection between consecutive frames (e.g. between It and It+1, between It+1 and It+2 and so on). Find how to normalize the intersection. Does that change the results? Plot the intersection values over time and the normalised intersection values, and save the corresponding figures. Show and comment the figures in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ae564-3336-448e-93a5-c24489491f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e185c-52f7-4bbe-a69b-3bbd6ea75059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_histogram_intersection(histogram1, histogram2):\n",
    "    # Normalize histograms\n",
    "    histogram1_normalized = histogram1 / sum(histogram1)\n",
    "    histogram2_normalized = histogram2 / sum(histogram2)\n",
    "\n",
    "    # Calculate histogram intersection\n",
    "    intersection = np.min(np.vstack((histogram1_normalized, histogram2_normalized)), axis=0)\n",
    "    return sum(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fedf2d0-6c8a-4ed1-9bd2-39bbd8521956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frame_intersection(video_path):\n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not capture.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    frame_lst = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_lst.append(frame)\n",
    "\n",
    "    # Release the video capture object\n",
    "    capture.release()\n",
    "\n",
    "    # Calculate intersection between consecutive frames\n",
    "    intersection_values = []\n",
    "    for i in range(len(frame_lst) - 1):\n",
    "        histogram1 = cv2.calcHist([frame_lst[i]], [0], None, [256], [0, 256])\n",
    "        histogram2 = cv2.calcHist([frame_lst[i+1]], [0], None, [256], [0, 256])\n",
    "        intersection = calculate_histogram_intersection(histogram1, histogram2)\n",
    "        intersection_values.append(intersection)\n",
    "\n",
    "    return intersection_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f0185-286a-49d3-b0ef-88889b399371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_intersection_values(intersection_values):\n",
    "    return np.array(intersection_values) / np.sum(intersection_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7759828-ef31-4dd8-8587-47b38a601c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_normalized_values(intersection_values, normalized_intersection_values):\n",
    "    # Create subplots in a 1x2 grid\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot line graph for intersection_values\n",
    "    axes[0].plot(intersection_values, label='Intersection Values')\n",
    "    axes[0].set(xlabel='Frame Number', ylabel='Intersection Value', title='Intersection Values over Time')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot line graph for normalized_intersection_values\n",
    "    axes[1].plot(normalized_intersection_values, label='Normalized Values', color='orange')\n",
    "    axes[1].set(xlabel='Frame Number', ylabel='Normalized Value', title='Normalized Values over Time')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "video_path = 'Dataset/DatasetB.avi'\n",
    "intersection_values = calculate_frame_intersection(video_path)\n",
    "normalized_intersection_values = normalize_intersection_values(intersection_values)\n",
    "plot_normalized_values(intersection_values, normalized_intersection_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d20f3-ade4-429f-910f-f8a47f362496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Function to compute histogram intersection and normalization\n",
    "def histogram_intersection(hist1, hist2):\n",
    "    intersection = np.sum(np.minimum(hist1, hist2))\n",
    "    normalization = intersection / np.sum(hist1)\n",
    "    return intersection, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d718b2-d683-4467-a7bb-dd0f0fd59b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_histogram(image):\n",
    "    histogram = cv2.calcHist([image], [0, 1, 2], None, [256, 256, 256], [0, 256, 0, 256, 0, 256])\n",
    "    histogram = histogram / histogram.sum()\n",
    "    return histogram.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5857237-2a58-418a-a496-adcba728c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_two_frames(lst):\n",
    "\n",
    "    frame1 = lst[0]\n",
    "    frame2 = lst[1]\n",
    "\n",
    "    # # Convert frames to BGR if not already\n",
    "    # if len(frame1.shape) == 2:\n",
    "    #     frame1 = cv2.cvtColor(frame1, cv2.COLOR_GRAY2BGR)\n",
    "    # if len(frame2.shape) == 2:\n",
    "    #     frame2 = cv2.cvtColor(frame2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Compute histogram for the current frames\n",
    "    hist1 = cv2.calcHist(frame1, [0], None, [256], [0, 256])\n",
    "    hist2 = cv2.calcHist(frame2, [0], None, [256], [0, 256])\n",
    "\n",
    "    # Compute intersection and normalization\n",
    "    intersection = calculate_histogram_intersection(hist1, hist2)\n",
    "    normalization = normalize_intersection_values(intersection)\n",
    "\n",
    "    # Plot intersection and normalized values\n",
    "    plot_normalized_values(intersection, normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f85359-73ab-4c9f-813f-80ddfe014c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = frame_lst[8:10]\n",
    "process_video_two_frames(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d30548-8d34-43b6-9f69-d360d9bd8007",
   "metadata": {},
   "source": [
    "### Texture Classification. (Use Datasets A and B)\n",
    "The Local Binary Pattern (LBP) operator describes the surroundings of a pixel by generating a bit-code \n",
    "from the binary derivatives of a pixel. \n",
    "#### a) Write a function that divides a greyscale image into equally sized non-overlapping windows and returns the feature descriptor for each window as distribution of LBP codes. For each pixel in the window, compare the pixel to each of its 8 neighbours. Convert the resulting bit-codes (base 2) to decimals (base 10 numbers) and compute their histogram over the window. Normalize the histogram (which is now a feature descriptor representing the window). Show in the report the resulting images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522479e-8cc9-4a55-aae4-d747dbf67492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145aaf2-cad3-43da-a2c1-a95729ef8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lbp_histogram(window):\n",
    "    \"\"\"\n",
    "    Compute the LBP histogram for a given window using vectorized operations.\n",
    "    \"\"\"\n",
    "    # Calculate LBP codes for each pixel in the window\n",
    "    lbp_codes = (window > np.roll(window, 1, axis=0)).astype(np.uint8)\n",
    "    lbp_codes = 2**7 * lbp_codes + (window > np.roll(window, 1, axis=(0, 1))).astype(np.uint8)\n",
    "    lbp_codes = 2**6 * lbp_codes + (window > np.roll(window, 0, axis=1)).astype(np.uint8)\n",
    "    lbp_codes = 2**5 * lbp_codes + (window > np.roll(window, -1, axis=(0, 1))).astype(np.uint8)\n",
    "    lbp_codes = 2**4 * lbp_codes + (window > np.roll(window, -1, axis=0)).astype(np.uint8)\n",
    "    lbp_codes = 2**3 * lbp_codes + (window > np.roll(window, -1, axis=(1, 0))).astype(np.uint8)\n",
    "    lbp_codes = 2**2 * lbp_codes + (window > np.roll(window, 0, axis=1)).astype(np.uint8)\n",
    "    lbp_codes = 2**1 * lbp_codes + (window > np.roll(window, 1, axis=(1, 0))).astype(np.uint8)\n",
    "    lbp_codes = lbp_codes + (window > window.mean()).astype(np.uint8)\n",
    "\n",
    "    # Convert to decimal values\n",
    "    decimal_values = np.packbits(lbp_codes.reshape(-1, 8)[:, ::-1], axis=1)\n",
    "\n",
    "    # Compute histogram\n",
    "    histogram, _ = np.histogram(decimal_values, bins=range(256), density=True)\n",
    "\n",
    "    return histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e45e58-bbf5-4463-9daf-370e5e23aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lbp_features(image, window_size):\n",
    "    \"\"\"\n",
    "    Extract LBP features for the given greyscale image.\n",
    "    \"\"\"\n",
    "    rows, cols = image.shape\n",
    "    lbp_features = []\n",
    "\n",
    "    for i in range(0, rows, window_size):\n",
    "        for j in range(0, cols, window_size):\n",
    "            window = image[i:i+window_size, j:j+window_size]\n",
    "\n",
    "            # Check if the window size is smaller than the specified size\n",
    "            if window.shape[0] < window_size or window.shape[1] < window_size:\n",
    "                continue\n",
    "\n",
    "            histogram = compute_lbp_histogram(window)\n",
    "            lbp_features.append(histogram)\n",
    "\n",
    "    return np.array(lbp_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c981e58-13b5-4bc6-adbf-d2e56ff0c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load greyscale image (replace 'image_path' with the actual file path)\n",
    "image_path = '/home/jovyan/Computer Vision/Dataset/DatasetA/face-1.jpg'\n",
    "gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Set the window size\n",
    "window_size = 16\n",
    "\n",
    "# Extract LBP features from the greyscale image\n",
    "lbp_features = extract_lbp_features(gray_image, window_size)\n",
    "\n",
    "# Plot the histogram for all windows\n",
    "plt.figure()\n",
    "for i in range(lbp_features.shape[0]):\n",
    "    plt.plot(lbp_features[i])\n",
    "\n",
    "plt.title('LBP Histograms for All Windows')\n",
    "plt.xlabel('Decimal Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84639aaa-7f5e-48ec-8b99-8600db1984fb",
   "metadata": {},
   "source": [
    "#### b) Come up with a descriptor that represents the whole image as consisting of multiple windows. For example, you could combine several local descriptions into a global description by concatenation.Discuss in the report alternative approaches. Using the global descriptor you created, implement a classification process that separates the images in the dataset into two categories: face images and non-face images (for example, you could use histogram similarities). Comment the results in the report. Is the global descriptor able to represent whole images of different types (e.g. faces vs. cars)? Identify problems (if any), discuss them in the report and suggest possible solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c235e5f-39fa-412c-ad14-4ba6dc01c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lbp_histogram(window):\n",
    "    \"\"\"\n",
    "    Compute the LBP histogram for a given window using vectorized operations.\n",
    "    \"\"\"\n",
    "    # Calculate LBP codes for each pixel in the window\n",
    "    lbp_codes = (window > np.roll(window, 1, axis=0)).astype(np.uint8)\n",
    "    lbp_codes = 2**7 * lbp_codes + (window > np.roll(window, 1, axis=(0, 1))).astype(np.uint8)\n",
    "    lbp_codes = 2**6 * lbp_codes + (window > np.roll(window, 0, axis=1)).astype(np.uint8)\n",
    "    lbp_codes = 2**5 * lbp_codes + (window > np.roll(window, -1, axis=(0, 1))).astype(np.uint8)\n",
    "    lbp_codes = 2**4 * lbp_codes + (window > np.roll(window, -1, axis=0)).astype(np.uint8)\n",
    "    lbp_codes = 2**3 * lbp_codes + (window > np.roll(window, -1, axis=(1, 0))).astype(np.uint8)\n",
    "    lbp_codes = 2**2 * lbp_codes + (window > np.roll(window, 0, axis=1)).astype(np.uint8)\n",
    "    lbp_codes = 2**1 * lbp_codes + (window > np.roll(window, 1, axis=(1, 0))).astype(np.uint8)\n",
    "    lbp_codes = lbp_codes + (window > window.mean()).astype(np.uint8)\n",
    "\n",
    "    # Convert to decimal values\n",
    "    decimal_values = np.packbits(lbp_codes.reshape(-1, 8)[:, ::-1], axis=1)\n",
    "\n",
    "    # Compute histogram\n",
    "    histogram, _ = np.histogram(decimal_values, bins=range(256), density=True)\n",
    "\n",
    "    return histogram\n",
    "\n",
    "\n",
    "\n",
    "def extract_lbp_features(image, window_size):\n",
    "    \"\"\"\n",
    "    Extract LBP features for the given greyscale image.\n",
    "    \"\"\"\n",
    "    rows, cols = image.shape\n",
    "    lbp_features = []\n",
    "\n",
    "    for i in range(0, rows, window_size):\n",
    "        for j in range(0, cols, window_size):\n",
    "            window = image[i:i+window_size, j:j+window_size]\n",
    "\n",
    "            # Check if the window size is smaller than the specified size\n",
    "            if window.shape[0] < window_size or window.shape[1] < window_size:\n",
    "                continue\n",
    "\n",
    "            histogram = compute_lbp_histogram(window)\n",
    "            lbp_features.append(histogram)\n",
    "\n",
    "    return np.array(lbp_features)\n",
    "\n",
    "def create_global_descriptor(local_descriptors):\n",
    "    \"\"\"\n",
    "    Create a global descriptor based on local descriptors.\n",
    "    \"\"\"\n",
    "    # Concatenate local descriptors to create a global descriptor\n",
    "    global_descriptor = np.concatenate(local_descriptors, axis=0)\n",
    "\n",
    "    # Normalize the global descriptor\n",
    "    global_descriptor /= np.linalg.norm(global_descriptor)\n",
    "\n",
    "    return global_descriptor\n",
    "\n",
    "# Example usage:\n",
    "# Load greyscale image for face and non-face (replace 'image_path' with the actual file paths)\n",
    "image_path_face = '/home/jovyan/Computer Vision/Dataset/DatasetA/face-1.jpg'\n",
    "image_path_non_face = '/home/jovyan/Computer Vision/Dataset/DatasetA/car-1.jpg'\n",
    "\n",
    "# Set the window size\n",
    "window_size = 16\n",
    "\n",
    "# Extract LBP features from the greyscale images\n",
    "lbp_features_face = extract_lbp_features(cv2.imread(image_path_face, cv2.IMREAD_GRAYSCALE), window_size)\n",
    "lbp_features_non_face = extract_lbp_features(cv2.imread(image_path_non_face, cv2.IMREAD_GRAYSCALE), window_size)\n",
    "\n",
    "# Create global descriptors for face and non-face images\n",
    "global_descriptor_face = create_global_descriptor(lbp_features_face)\n",
    "global_descriptor_non_face = create_global_descriptor(lbp_features_non_face)\n",
    "\n",
    "# Load an image to be classified (replace 'test_image_path' with the actual file path)\n",
    "test_image_path = '/home/jovyan/Computer Vision/Dataset/DatasetA/car-2.jpg'\n",
    "lbp_features_test = extract_lbp_features(cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE), window_size)\n",
    "global_descriptor_test = create_global_descriptor(lbp_features_test)\n",
    "\n",
    "# Simple classification process\n",
    "# Assume a threshold value for classification\n",
    "threshold = 0.7\n",
    "\n",
    "# Calculate similarity for face\n",
    "similarity_face = np.dot(global_descriptor_test, global_descriptor_face) / (np.linalg.norm(global_descriptor_test) * np.linalg.norm(global_descriptor_face))\n",
    "\n",
    "# Calculate similarity for non-face\n",
    "similarity_non_face = np.dot(global_descriptor_test, global_descriptor_non_face) / (np.linalg.norm(global_descriptor_test) * np.linalg.norm(global_descriptor_non_face))\n",
    "\n",
    "# Classification process is based on histogram similarities\n",
    "if similarity_face > similarity_non_face and similarity_face > threshold:\n",
    "    print(\"Image: Face\")\n",
    "elif similarity_non_face > similarity_face and similarity_non_face > threshold:\n",
    "    print(\"Image: Non-Face\")\n",
    "else:\n",
    "    print(\"Image: Unknown\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9cec79-3cf1-4973-bfed-16a72eea4171",
   "metadata": {},
   "source": [
    "#### c) Decrease the window size and perform classification again. Comment the results in the report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1df10-2207-414c-8125-1b2b2aade23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_lbp_features(image, window_size):\n",
    "    \"\"\"\n",
    "    Extract LBP features for the given image.\n",
    "    \"\"\"\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    rows, cols = gray_image.shape\n",
    "    lbp_features = []\n",
    "\n",
    "    for i in range(0, rows, window_size):\n",
    "        for j in range(0, cols, window_size):\n",
    "            window = gray_image[i:i+window_size, j:j+window_size]\n",
    "\n",
    "            # Check if the window size is smaller than the specified size\n",
    "            if window.shape[0] < window_size or window.shape[1] < window_size:\n",
    "                continue\n",
    "\n",
    "            # Compute LBP histogram for the window\n",
    "            histogram = compute_lbp_histogram(window)\n",
    "            lbp_features.append(histogram)\n",
    "\n",
    "    return np.array(lbp_features)\n",
    "\n",
    "# Load face image\n",
    "face_image = cv2.imread(image_path_face)\n",
    "if face_image is None:\n",
    "    raise ValueError(f\"Failed to load image: {image_path_face}\")\n",
    "\n",
    "# Load non-face image\n",
    "non_face_image = cv2.imread(image_path_non_face)\n",
    "if non_face_image is None:\n",
    "    raise ValueError(f\"Failed to load image: {image_path_non_face}\")\n",
    "\n",
    "# Load an image to be classified\n",
    "test_image = cv2.imread(test_image_path)\n",
    "if test_image is None:\n",
    "    raise ValueError(f\"Failed to load image: {test_image_path}\")\n",
    "\n",
    "# Set the window size\n",
    "window_size = 32\n",
    "window_size_small = 12\n",
    "\n",
    "# Extract LBP features from the color images\n",
    "lbp_features_face = extract_lbp_features(face_image, window_size)\n",
    "lbp_features_non_face = extract_lbp_features(non_face_image, window_size)\n",
    "\n",
    "lbp_features_face_small = extract_lbp_features(face_image, window_size_small)\n",
    "lbp_features_non_face_small = extract_lbp_features(non_face_image, window_size_small)\n",
    "\n",
    "# Create global descriptors for face and non-face images\n",
    "global_descriptor_face = create_global_descriptor(lbp_features_face)\n",
    "global_descriptor_non_face = create_global_descriptor(lbp_features_non_face)\n",
    "\n",
    "global_descriptor_face_small = create_global_descriptor(lbp_features_face_small)\n",
    "global_descriptor_non_face_small = create_global_descriptor(lbp_features_non_face_small)\n",
    "\n",
    "# Extract LBP features from the test image\n",
    "lbp_features_test = extract_lbp_features(test_image, window_size)\n",
    "lbp_features_test_small = extract_lbp_features(test_image, window_size_small)\n",
    "\n",
    "# Create global descriptors for the test image\n",
    "global_descriptor_test = create_global_descriptor(lbp_features_test)\n",
    "global_descriptor_test_small = create_global_descriptor(lbp_features_test_small)\n",
    "\n",
    "# Assume a threshold value for classification\n",
    "threshold = 0.4\n",
    "\n",
    "# Calculate similarity for face\n",
    "similarity_face = np.dot(global_descriptor_test, global_descriptor_face) / (np.linalg.norm(global_descriptor_test) * np.linalg.norm(global_descriptor_face))\n",
    "\n",
    "# Calculate similarity for non-face\n",
    "similarity_non_face = np.dot(global_descriptor_test, global_descriptor_non_face) / (np.linalg.norm(global_descriptor_test) * np.linalg.norm(global_descriptor_non_face))\n",
    "\n",
    "# Classification process is based on histogram similarities\n",
    "if similarity_face > threshold:\n",
    "    print(\"Image: Face\")\n",
    "else:\n",
    "    print(\"Image: Non-Face\")\n",
    "\n",
    "# Perform the classification with the decreased window size\n",
    "# Calculate similarity for face (small window size)\n",
    "similarity_face_small = np.dot(global_descriptor_test_small, global_descriptor_face_small) / (np.linalg.norm(global_descriptor_test_small) * np.linalg.norm(global_descriptor_face_small))\n",
    "\n",
    "# Calculate similarity for non-face (small window size)\n",
    "similarity_non_face_small = np.dot(global_descriptor_test_small, global_descriptor_non_face_small) / (np.linalg.norm(global_descriptor_test_small) * np.linalg.norm(global_descriptor_non_face_small))\n",
    "\n",
    "# Classification process is based on histogram similarities with the decreased window size\n",
    "if similarity_face_small > threshold:\n",
    "    print(\"Image (small window size): Face\")\n",
    "else:\n",
    "    print(\"Image (small window size): Non-Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f06ca72-29b7-4a8b-a0e7-43eebaf5be2d",
   "metadata": {},
   "source": [
    "#### d) Increase the window size and perform classification again. Comment the results in the report.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be45480-91ee-493c-a922-f3e565811eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_lbp_features(image, window_size):\n",
    "    \"\"\"\n",
    "    Extract LBP features for the given image.\n",
    "    \"\"\"\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    rows, cols = gray_image.shape\n",
    "    lbp_features = []\n",
    "\n",
    "    for i in range(0, rows, window_size):\n",
    "        for j in range(0, cols, window_size):\n",
    "            window = gray_image[i:i+window_size, j:j+window_size]\n",
    "\n",
    "            # Check if the window size is smaller than the specified size\n",
    "            if window.shape[0] < window_size or window.shape[1] < window_size:\n",
    "                continue\n",
    "\n",
    "            # Compute LBP histogram for the window\n",
    "            histogram = compute_lbp_histogram(window)\n",
    "            lbp_features.append(histogram)\n",
    "\n",
    "    return np.array(lbp_features)\n",
    "\n",
    "# Load face image\n",
    "face_image = cv2.imread(image_path_face)\n",
    "if face_image is None:\n",
    "    raise ValueError(f\"Failed to load image: {image_path_face}\")\n",
    "\n",
    "# Load non-face image\n",
    "non_face_image = cv2.imread(image_path_non_face)\n",
    "if non_face_image is None:\n",
    "    raise ValueError(f\"Failed to load image: {image_path_non_face}\")\n",
    "\n",
    "# Load an image to be classified\n",
    "test_image = cv2.imread(test_image_path)\n",
    "if test_image is None:\n",
    "    raise ValueError(f\"Failed to load image: {test_image_path}\")\n",
    "\n",
    "# Set the window size\n",
    "window_size = 32\n",
    "window_size_large = 12\n",
    "\n",
    "# Extract LBP features from the color images\n",
    "lbp_features_face = extract_lbp_features(face_image, window_size)\n",
    "lbp_features_non_face = extract_lbp_features(non_face_image, window_size)\n",
    "\n",
    "lbp_features_face_large = extract_lbp_features(face_image, window_size_large)\n",
    "lbp_features_non_face_large = extract_lbp_features(non_face_image, window_size_large)\n",
    "\n",
    "# Create global descriptors for face and non-face images\n",
    "global_descriptor_face = create_global_descriptor(lbp_features_face)\n",
    "global_descriptor_non_face = create_global_descriptor(lbp_features_non_face)\n",
    "\n",
    "global_descriptor_face_large = create_global_descriptor(lbp_features_face_large)\n",
    "global_descriptor_non_face_large = create_global_descriptor(lbp_features_non_face_large)\n",
    "\n",
    "# Extract LBP features from the test image\n",
    "lbp_features_test = extract_lbp_features(test_image, window_size)\n",
    "lbp_features_test_large = extract_lbp_features(test_image, window_size_large)\n",
    "\n",
    "# Create global descriptors for the test image\n",
    "global_descriptor_test = create_global_descriptor(lbp_features_test)\n",
    "global_descriptor_test_large = create_global_descriptor(lbp_features_test_large)\n",
    "\n",
    "# Assume a threshold value for classification\n",
    "threshold = 0.4\n",
    "\n",
    "# Calculate similarity for face\n",
    "similarity_face = np.dot(global_descriptor_test, global_descriptor_face) / (np.linalg.norm(global_descriptor_test) * np.linalg.norm(global_descriptor_face))\n",
    "\n",
    "# Calculate similarity for non-face\n",
    "similarity_non_face = np.dot(global_descriptor_test, global_descriptor_non_face) / (np.linalg.norm(global_descriptor_test) * np.linalg.norm(global_descriptor_non_face))\n",
    "\n",
    "# Classification process is based on histogram similarities\n",
    "if similarity_face > threshold:\n",
    "    print(\"Image: Face\")\n",
    "else:\n",
    "    print(\"Image: Non-Face\")\n",
    "\n",
    "# Perform the classification with the decreased window size\n",
    "# Calculate similarity for face (large window size)\n",
    "similarity_face_large = np.dot(global_descriptor_test_large, global_descriptor_face_large) / (np.linalg.norm(global_descriptor_test_large) * np.linalg.norm(global_descriptor_face_large))\n",
    "\n",
    "# Calculate similarity for non-face (large window size)\n",
    "similarity_non_face_large = np.dot(global_descriptor_test_large, global_descriptor_non_face_large) / (np.linalg.norm(global_descriptor_test_large) * np.linalg.norm(global_descriptor_non_face_large))\n",
    "\n",
    "# Classification process is based on histogram similarities with the decreased window size\n",
    "if similarity_face_large > threshold:\n",
    "    print(\"Image (large window size): Face\")\n",
    "else:\n",
    "    print(\"Image (large window size): Non-Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d05055e-24da-4320-947e-ff505a56c372",
   "metadata": {},
   "source": [
    "### 5) Object Counting. (Use Dataset C)\n",
    "Moving objects captured by fixed cameras are the focus of several computer vision applications.\n",
    "#### a) Write a function that performs pixel-by-pixel frame differencing using, as reference frame, the first frame of an image sequence. Apply a classification threshold and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b9e97-544d-45cd-9d40-70bcacf4f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def frame_difference(video_path, output_path, threshold=30):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Read the first frame as the reference frame\n",
    "    ret, reference_frame = cap.read()\n",
    "\n",
    "    # Convert the reference frame to grayscale\n",
    "    reference_gray = cv2.cvtColor(reference_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # List to store frames\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        # Read a new frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute the absolute difference between the current frame and the reference frame\n",
    "        frame_diff = cv2.absdiff(reference_gray, frame_gray)\n",
    "\n",
    "        # Apply a threshold to classify pixels as moving or static\n",
    "        _, thresholded = cv2.threshold(frame_diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Save the result\n",
    "        frames.append(thresholded)\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "\n",
    "    # Display frames in Jupyter notebook\n",
    "    for i, frame in enumerate(frames):\n",
    "        image_data = cv2.imencode('.png', frame)[1].tobytes()\n",
    "        display(Image(data=image_data, format='png'))\n",
    "\n",
    "# Example usage\n",
    "video_path = 'Dataset/DatasetC.avi'\n",
    "output_path = 'Dataset/DatasetC_result.avi'\n",
    "frame_difference(video_path, output_path, threshold=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4d4e7-465d-438d-83a4-afa02db0248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def frame_difference(video_path, output_path, threshold=30):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Read the first frame as the initial reference frame\n",
    "    ret, reference_frame = cap.read()\n",
    "\n",
    "    # Convert the initial reference frame to grayscale\n",
    "    reference_gray = cv2.cvtColor(reference_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Define the codec and create VideoWriter object for saving the result\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 20.0, (reference_frame.shape[1], reference_frame.shape[0]))\n",
    "\n",
    "    while True:\n",
    "        # Read a new frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute the absolute difference between the current frame and the previous frame\n",
    "        frame_diff = cv2.absdiff(reference_gray, frame_gray)\n",
    "\n",
    "        # Apply a threshold to classify pixels as moving or static\n",
    "        _, thresholded = cv2.threshold(frame_diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Save the result\n",
    "        out.write(thresholded)\n",
    "\n",
    "        # Update the reference frame for the next iteration\n",
    "        reference_gray = frame_gray\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    \n",
    "    out.release()\n",
    "\n",
    "# Example usage\n",
    "video_path = 'Dataset/DatasetC.avi'\n",
    "output_path = 'Dataset/DatasetC_result_using_previous_frame.avi'\n",
    "frame_difference(video_path, output_path, threshold=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500d3ea-b4c1-4927-93e2-4183d01c5a40",
   "metadata": {},
   "source": [
    "#### b) Repeat the exercise using the previous frame as reference frame (use frame It-1 as reference frame for frame It, for each t). Comment the results in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72468e9-54e6-4a6d-a5a9-e082e33f0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import display, Image\n",
    "import io\n",
    "\n",
    "def frame_difference(video_path, threshold=30):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Read the first frame as the initial reference frame\n",
    "    ret, reference_frame = cap.read()\n",
    "\n",
    "    # Convert the initial reference frame to grayscale\n",
    "    reference_gray = cv2.cvtColor(reference_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # List to store frames\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        # Read a new frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute the absolute difference between the current frame and the previous frame\n",
    "        frame_diff = cv2.absdiff(reference_gray, frame_gray)\n",
    "\n",
    "        # Apply a threshold to classify pixels as moving or static\n",
    "        _, thresholded = cv2.threshold(frame_diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Display the result (optional)\n",
    "        image_data = cv2.imencode('.png', thresholded)[1].tobytes()\n",
    "        display(Image(data=image_data, format='png'))\n",
    "\n",
    "        # Update the reference frame for the next iteration\n",
    "        reference_gray = frame_gray\n",
    "\n",
    "# Example usage\n",
    "video_path = 'Dataset/DatasetC.avi'\n",
    "frame_difference(video_path, threshold=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646e162-85c5-49f2-846a-8dd8a7303df1",
   "metadata": {},
   "source": [
    "#### c) Write a function that generates a reference frame (background) for the sequence using for example frame differencing and a weighted temporal averaging algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad2f25-cfce-4129-972c-540a56394ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def generate_reference_frame(video_path, alpha=0.01, threshold=30):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Read the first frame as the initial reference frame\n",
    "    ret, reference_frame = cap.read()\n",
    "\n",
    "    # Convert the initial reference frame to grayscale\n",
    "    reference_gray = cv2.cvtColor(reference_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize the accumulated frame for weighted temporal averaging\n",
    "    accumulated_frame = np.float32(reference_gray)\n",
    "\n",
    "    # Process each frame in the video sequence\n",
    "    while True:\n",
    "        # Read a new frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute the absolute difference between the current frame and the accumulated frame\n",
    "        frame_diff = cv2.absdiff(accumulated_frame.astype(np.uint8), frame_gray)\n",
    "\n",
    "        # Apply a binary threshold to classify pixels as moving or static\n",
    "        _, thresholded = cv2.threshold(frame_diff, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "\n",
    "        # Weighted temporal averaging algorithm\n",
    "        cv2.accumulateWeighted(frame_gray, accumulated_frame, alpha)\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "\n",
    "    # Convert the accumulated frame to uint8\n",
    "    reference_frame_result = cv2.convertScaleAbs(accumulated_frame)\n",
    "\n",
    "    return reference_frame_result\n",
    "\n",
    "def display_image(image, title=\"Image\"):\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "video_path = 'Dataset/DatasetC.mpg'\n",
    "reference_frame = generate_reference_frame(video_path, alpha=0.01, threshold=50)\n",
    "\n",
    "\n",
    "\n",
    "# Display the generated reference frame\n",
    "display_image(reference_frame, title='Generated Reference Frame')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6e08a-6006-4570-8ece-b04ca5c22d5a",
   "metadata": {},
   "source": [
    "#### d) Write a function that counts the number of moving objects in each frame of a sequence. Generate a bar plot that visualizes the number of objects for each frame of the whole sequence. Discuss in the report the implemented solution, including advantages and disadvantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09863605-a3c1-4ca0-85c4-3f9b09dc5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_moving_objects(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    object_counts = []\n",
    "\n",
    "    # Create background subtractor\n",
    "    bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "    for _ in range(frame_count):\n",
    "        _, frame = cap.read()\n",
    "\n",
    "        # Apply background subtraction\n",
    "        fg_mask = bg_subtractor.apply(frame)\n",
    "\n",
    "        # Threshold the mask\n",
    "        _, thresh = cv2.threshold(fg_mask, 9, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Find contours of moving objects\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Count the number of moving objects in the current frame\n",
    "        object_count = len(contours)\n",
    "        object_counts.append(object_count)\n",
    "\n",
    "    cap.release()\n",
    "    return object_counts\n",
    "\n",
    "def plot_object_counts(object_counts):\n",
    "    # Increase the figure size\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.bar(range(1, len(object_counts) + 1), object_counts)\n",
    "    plt.xlabel('Frame Number')\n",
    "    plt.ylabel('Number of Moving Objects')\n",
    "    plt.title('Number of Moving Objects in Each Frame')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "video_path = 'Dataset/DatasetC.mpg'\n",
    "object_counts = count_moving_objects(video_path)\n",
    "plot_object_counts(object_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
